<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no, shrink-to-fit=no">
        <meta name="description" content="Gaotian Wang">
        <meta name="author" content="Gaotian Wang">
        <title>Gaotian Wang</title>
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
        <link href="https://fonts.googleapis.com/css?family=Roboto:400,400i,500,500i" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Zilla+Slab:600i" rel="stylesheet">
        <script defer src="https://use.fontawesome.com/releases/v6.5.1/js/all.js"></script>
        <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
        <link rel="icon" href="images/robot.svg" type="image/png">
        <style>
         #sticky-sidebar {
             position:fixed;
             max-width: 20%;
         }
         html {
             height: 100%;
             width : 100%;
         }
         body {
             height     : 100%;
             width      : 100%;
             font-family: 'Trebuchet MS', sans-serif;
             font-weight: 300;
             padding-top: 4.5rem;
         }
         b {
             font-weight: 500;
         }
         h1, h2, h3, .navbar-brand {
             font-family: 'Zilla Slab', serif;
             font-weight: 600;
             color: #0f4c97;
         }
         h3 {
         }
         a {
             color: #0f4c97;
         }
         a:hover {
             color: #1d83ff;
             text-decoration: none;
         }
         a.anchor {
             display: block;
             position: relative;
             top: -70px;
             visibility: hidden;
         }
         .title {
             font-size: 110%;
             font-style: italic;
         }
         .book {
             font-size: 110%;
         }
         well {
             margin: 0 auto;
             width: 100%;
         }
         .bg-light {
             background-color: rgba(213, 220, 255, 0.8) !important;
         }
        </style>
    </head>

    <body data-spy="scroll" data-target=".navbar">
        <nav class="navbar navbar-expand-lg navbar-light fixed-top bg-light">
            <a class="navbar-brand" href="#" style="font-size: 1.5em; color: #0f4c97">Gaotian Wang</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav mr-auto">
                    <li class="nav-item">
                        <a class="nav-link current" href="#about"><i class="fa fa-child"></i> About</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#news"><i class="fa fa-newspaper"></i> News</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#experience"><i class="fa fa-school"></i> Experience</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#projects"><i class="fa fa-code"></i> Projects</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#pubs"><i class="fa fa-pen"></i> Publications</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#teaching"><i class="fa fa-chalkboard"></i> Teaching</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#Hobbies"><i class="fa fa-mountain"></i> Life</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="https://robotpilab.github.io/"><i class="fa fa-robot"></i> Lab</a>
                    </li>
                    
                </ul>
            </div>
        </nav>
        <div class="container mt-4">
            <div class="row">
                <div class="col-12 col-xs-8 col-md-9 col-lg-9 col-xl-10 order-last" style="border: 20px solid transparent;">
                    <a class="anchor" id="about"></a>
                    <h1>About üßë‚Äçüíª</h1>
                    <div style="padding: 10px">
                        <p>
                            I am a graduate student at Rice University, focusing on robot manipulation (embodied AI) and motion planning 
                            in the <a href="https://robotpilab.github.io/">Robot&Pi; lab</a>, under the direction of Professor 
                            <a href="https://hangkaiyu.github.io/">Kaiyu Hang</a>. 
                        </p>
                        
                        <p>
                            My recent interest lies in <nobr style="font-weight: bold;">robust nonprehensile manipulation</nobr>, where we propse <nobr style="font-weight: bold;">energy-based virtual temporal cages</nobr> and <nobr style="font-weight: bold;">funnels</nobr> for everyday tasks to achieve robust manipulation under all kinds of uncertainties and provides solutions to <nobr style="font-weight: bold;">close the Sim2real gap</nobr>.
                            I am also interested in building <nobr style="font-weight: bold;">physics-aware multi-modal world models</nobr> that can predict the potential motions of objects to reach general robust manipulation. 
                            
                        </p>
                        
                        <p>
                            More details can be found in my <a href="Gaotian_Wang_CV.pdf">curriculum vitae <i class="fa fa-file-pdf"></i></a>.
                        </p>
                    </div>
                    <hr>
                    <a class="anchor" id="news"></a>
                    <h1>News üì∞</h1>
					<div style="padding: 10px">
                    <dl>
                        <dt>IJRR</dt>
                            <dd>
                                <p><a href="#wang_caging_2024">Caging in Time: A Framework for Robust Object Manipulation under Uncertainties and Limited Robot Perception</a> is accepted to IJRR. GO take a look!         
                                </p>
                            </dd>
                        
                    </dl>
                    </div>
                    <div style="padding: 10px">
                    <dl>
                        <dt><a href="https://github.com/Vector-Wangel/XLeRobot">XLeRobot</a></dt>
                            <dd>
                                <p>
                                    <a href="https://github.com/Vector-Wangel/XLeRobot">XLeRobot</a> is a fully open-sourced practical low-cost household dual-arm mobile robot for general manipulation, with $660 cost and less than 4 hours total assembly time. Parts available around the world. Fully based on LeRobot, easy access to vast LeRobot code and AI models, interact with 6000+ (rapidly growing) community members. <a href="https://github.com/Vector-Wangel/XLeRobot">Go take a look!</a>
                                    <div class="text-center mt-3">
                                        <img src="images/XLeRobot.jpg" alt="XLeRobot" class="img-fluid" style="max-width: 100%; height: auto;">
                                        <p class="text-muted mt-2">XLeRobot: A low-cost dual-arm mobile manipulation platform</p>
                                    </div>
                                </p>
                            </dd>
                        
                    </dl>
                    </div>
					
                    <hr>

                    <!-- Projects Section -->                
                    <a class="anchor" id="projects"></a>
                    <h1>Projects ü¶æ</h1>
                    <div style="padding: 10px">
                        <dl>

                            <div class="row">
                                <!-- Project Column -->
                                <div class="col-md-6">
                                    <dt style="font-size: 1.12em;">Caging in Time for Robust Object Manipulation </dt>
                                    <ul>
                                        <li>A system model that propagates the Potential State Set(PSS) over time and construct caging configurations in time for robust manipulation.</li>
                                        <li>Instantiated on both quasi-static and dynamic non-prehensile manipulation tasks in an open-loop manner without specific object properties known a priori</li>
                                    </ul>
                                </div>
                                <!-- Video Column -->
                                <div class="col-md-6">
                                    <video width="420" height="240" controls>
                                        <source src="videos/CIT_IJRR.mp4" type="video/mp4">
                                        Your browser does not support the video tag.
                                    </video>
                                </div>
                            </div>
                            <div class="row">
                                <!-- Project Column -->
                                <div class="col-md-6">
                                    <dt style="font-size: 1.12em;">XLeRobot: A Practical Low-cost Household</dt>
                                    <dt style="font-size: 1.12em;">Dual-Arm Mobile Robot for General Manipulation</dt>
                                    <ul>
                                        <li>XLeRobot = XL LeRobot = Lekiwi base + SO101 arm + IKEA cart + Anker power supply. $660 cost and less than 4 hours total assembly time.</li>
                                        <li>Fully based on LeRobot, easy access to vast LeRobot code and AI models, interact with 6000+ (rapidly growing) community members.</li>
                                    </ul>
                                </div>
                                <!-- Video Column -->
                                <div class="col-md-6">
                                    <video width="420" height="240" controls>
                                        <source src="videos/Demo_0_1_0.mp4" type="video/mp4">
                                        Your browser does not support the video tag.
                                    </video>
                                </div>
                            </div>
                            
                            
                        </dl>
                    </div>
                    <hr>

                    <a class="anchor" id="pubs"></a>
                    <h1>Publications üì∞</h1>

                    <div style="padding: 10px">
                        <a id="journals" class="anchor"></a>
                        <h2>Peer-Reviewed Journal Articles</h2>
                        <div>
                            <dl class="row">
                                <h3 class="col-lg-12 col-xl-1">2025</h3>

                    
                                
                                <dt class="col-1" style="text-align:right;">J4.</dt>
                                <dd class="col-md-11 col-xl-10">
                                    <div class="row">
                                        <!-- Main content column (4/5 width) -->
                                        <div class="col-8">
                                            <a class="anchor" id="wang_caging_2024"></a>
                                            <p>
                                                <font class="title">Caging in Time: A Framework for Robust Object Manipulation under Uncertainties and Limited Robot Perception</font>
                                                <br>
                                                <nobr style="font-weight: bold;">Gaotian Wang</nobr>, <nobr>Kejia Ren</nobr>, <nobr>Andrew S. Morgan</nobr>, <nobr>Kaiyu Hang</nobr>
                                                <br>
                                                <em>International Journal of Robotics Research (IJRR)</em>
                                            </p>
                                            <p>
                                                <a href="#2410.16481bibtex" data-toggle="collapse">Bibtex</a> <b>/ </b>
                                                <a href="#2410.16481abstract" data-toggle="collapse">Abstract</a> <b>/ </b>
                                                <a href="https://arxiv.org/abs/2410.16481"><i class="fa fa-file-pdf"></i> PDF</a> <b>/ </b>
                                                <a href="publications/Caging_in_Time_zh.pdf"><i class="fa fa-file-pdf"></i> ‰∏≠Êñá</a> <b>/ </b>
                                                <a href="publications/Caging_in_Time_Poster.pdf"><i class="fa fa-file-image"></i> Poster</a> <b>/ </b>
                                                <a href="https://www.youtube.com/watch?v=Ag_jTzazuSM"><i class="fa fa-video"></i> Video</a>
                                            </p>
                                            <div id="2410.16481abstract" class="collapse">
                                                <div class="well">
                                                    <p>Real-world object manipulation has been commonly challenged by physical uncertainties and perception limitations. Being an effective strategy, while caging configuration-based manipulation frameworks have successfully provided robust solutions, they are not broadly applicable due to their strict requirements on the availability of multiple robots, widely distributed contacts, or specific geometries of the robots or the objects. To this end, this work proposes a novel concept, termed Caging in Time, to allow caging configurations to be formed even if there is just one robot engaged in a task. This novel concept can be explained by an insight that even if a caging configuration is needed to constrain the motion of an object, only a small portion of the cage is actively manipulating at a time. As such, we can switch the configuration of the robot strategically so that by collapsing its configuration in time, we will see a cage formed and its necessary portion active whenever needed. We instantiate our Caging in Time theory on challenging quasistatic and dynamic manipulation tasks, showing that Caging in Time can be achieved in general state spaces including geometry-based and energy-based spaces. With extensive experiments, we show robust and accurate manipulation, in an open-loop manner, without requiring detailed knowledge of the object geometry or physical properties, nor realtime accurate feedback on the manipulation states. In addition to being an effective and robust open-loop manipulation solution, the proposed theory can be a supplementary strategy to other manipulation systems affected by uncertain or limited robot perception.</p>
                                                </div>
                                                <p><a href="#2410.16481abstract" data-toggle="collapse">Close</a></p>
                                            </div>
                                            <div id="2410.16481bibtex" class="collapse">
                                                <div class="well">
                                                    <pre>@article{wang2024caging,
                            title={Caging in Time: A Framework for Robust Object Manipulation under Uncertainties and Limited Robot Perception},
                            author={Wang, Gaotian and Ren, Kejia and Morgan, Andrew S. and Hang, Kaiyu},
                            journal={arXiv preprint arXiv:2410.16481},
                            year={2024}
                        }
                                                    </pre>
                                                </div>
                                            </div>
                                        </div>
                                        <!-- GIF column (1/5 width) -->
                                        <div class="col-4">
                                            <img src="images/cage.gif" 
                                                 class="img-fluid" 
                                                 alt="Caging in Time demonstration"
                                                 style="width: 100%; margin-top: 0px;">
                                        </div>
                                    </div>
                                </dd>
                            </dl>
                        </div>





                        <div>
                            <dl class="row">
                                <h3 class="col-lg-12 col-xl-1">2024</h3>

                                <dt class="col-1" style="text-align:right;">J3.</dt>
                                <dd class="col-md-11 col-xl-10">
                                    <div class="row">
                                        <!-- Main content column (4/5 width) -->
                                        <div class="col-8">
                                            <p>
                                                <font class="title">Collision-inclusive Manipulation Planning for Occluded Object Grasping via Compliant Robot Motions</font>
                                                <br>
                                                <nobr>Kejia Ren</nobr>, <nobr style="font-weight: bold;">Gaotian Wang</nobr>, <nobr>Andrew S. Morgan</nobr>, <nobr>Kaiyu Hang</nobr>
                                                <br>
                                                <em>arXiv preprint</em>
                                            </p>
                                            <p>
                                                <a href="#ren2024collisionbibtex" data-toggle="collapse">Bibtex</a> <b>/ </b>
                                                <a href="#ren2024collisionabstract" data-toggle="collapse">Abstract</a> <b>/ </b>
                                                <a href="https://arxiv.org/pdf/2412.06983.pdf"><i class="fa fa-file-pdf"></i> PDF</a> <b>/ </b>
                                                <a href="https://www.youtube.com/watch?v=NxHyLqYKbPY"><i class="fa fa-video"></i> Video</a>
                                            </p>
                                            <div id="ren2024collisionabstract" class="collapse">
                                                <div class="well">
                                                    <p>Traditional robotic manipulation mostly focuses on collision-free tasks. In practice, however, many manipulation tasks (e.g., occluded object grasping) require the robot to intentionally collide with the environment to reach a desired task configuration. By enabling compliant robot motions, collisions between the robot and the environment are allowed and can thus be exploited, but more physical uncertainties are introduced. To address collision-rich problems such as occluded object grasping while handling the involved uncertainties, we propose a collision-inclusive planning framework that can transition the robot to a desired task configuration via roughly modeled collisions absorbed by Cartesian impedance control. By strategically exploiting the environmental constraints and exploring inside a manipulation funnel formed by task repetitions, our framework can effectively reduce physical and perception uncertainties. With real-world evaluations on both single-arm and dual-arm setups, we show that our framework is able to efficiently address various realistic occluded grasping problems where a feasible grasp does not initially exist.</p>
                                                </div>
                                                <p><a href="#ren2024collisionabstract" data-toggle="collapse">Close</a></p>
                                            </div>
                                            <div id="ren2024collisionbibtex" class="collapse">
                                                <div class="well">
                                                    <pre>@misc{ren2024collisioninclusivemanipulationplanningoccluded,
                                    title={Collision-inclusive Manipulation Planning for Occluded Object Grasping via Compliant Robot Motions}, 
                                    author={Kejia Ren and Gaotian Wang and Andrew S. Morgan and Kaiyu Hang},
                                    year={2024},
                                    eprint={2412.06983},
                                    archivePrefix={arXiv},
                                    primaryClass={cs.RO},
                                    url={https://arxiv.org/abs/2412.06983}, 
                                }</pre>
                                                </div>
                                            </div>
                                        </div>
                                        <!-- GIF column (1/5 width) -->
                                        <div class="col-4">
                                            <img src="images/collision.gif" 
                                                class="img-fluid" 
                                                alt="Collision-inclusive Planning demonstration"
                                                style="width: 100%; margin-top: 0px;">
                                        </div>
                                    </div>
                                </dd>
                                <h3 class="col-lg-12 col-xl-1"></h3>
                                <dt class="col-1" style="text-align:right;">J2.</dt>
                                <dd class="col-md-11 col-xl-10">
                                    <div class="row">
                                        <!-- Main content column (4/5 width) -->
                                        <div class="col-8">
                                            <p>
                                                <font class="title">Object-Centric Kinodynamic Planning for Nonprehensile Robot Rearrangement Manipulation</font>
                                                <br>
                                                <nobr>Kejia Ren</nobr>, <nobr style="font-weight: bold;">Gaotian Wang</nobr>, <nobr>Andrew S. Morgan</nobr>, <nobr>Lydia E. Kavraki</nobr>, <nobr>Kaiyu Hang</nobr>
                                                <br>
                                                <em>arXiv preprint</em>
                                            </p>
                                            <p>
                                                <a href="#ren2024objectcentricbibtex" data-toggle="collapse">Bibtex</a> <b>/ </b>
                                                <a href="#ren2024objectcentricabstract" data-toggle="collapse">Abstract</a> <b>/ </b>
                                                <a href="https://arxiv.org/abs/2410.00261"><i class="fa fa-file-pdf"></i> PDF</a> <b>/ </b>
                                                <a href="https://www.youtube.com/watch?v=PV2Y5SLSZkQ"><i class="fa fa-video"></i> Video</a>
                                            </p>
                                            <div id="ren2024objectcentricabstract" class="collapse">
                                                <div class="well">
                                                    <p>Nonprehensile actions such as pushing are crucial for addressing multi-object rearrangement problems. To date, existing nonprehensile solutions are all robot-centric, i.e., the manipulation actions are generated with robot-relevant intent and their outcomes are passively evaluated afterwards. Such pipelines are very different from human strategies and are typically inefficient. To this end, this work proposes a novel object-centric planning paradigm and develops the first object-centric planner for general nonprehensile rearrangement problems. By assuming that each object can actively move without being driven by robot interactions, the object-centric planner focuses on planning desired object motions, which are realized via robot actions generated online via a closed-loop pushing strategy. Through extensive experiments and in comparison with state-of-the-art baselines in both simulation and on a physical robot, we show that our object-centric paradigm can generate more intuitive and task-effective robot actions with significantly improved efficiency. In addition, we propose a benchmarking protocol to standardize and facilitate future research in nonprehensile rearrangement.</p>
                                                </div>
                                                <p><a href="#ren2024objectcentricabstract" data-toggle="collapse">Close</a></p>
                                            </div>
                                            <div id="ren2024objectcentricbibtex" class="collapse">
                                                <div class="well">
                                                    <pre>@misc{ren2024objectcentric,
                            title={Object-Centric Kinodynamic Planning for Nonprehensile Robot Rearrangement Manipulation},
                            author={Kejia Ren and Gaotian Wang and Andrew S. Morgan and Lydia E. Kavraki and Kaiyu Hang},
                            year={2024},
                            eprint={2410.00261},
                            archivePrefix={arXiv},
                            primaryClass={cs.RO}
                        }</pre>
                                                </div>
                                            </div>
                                        </div>
                                        <!-- GIF column (1/5 width) -->
                                        <div class="col-4">
                                            <img src="images/unosort.gif" 
                                                 class="img-fluid" 
                                                 alt="Object-Centric Planning demonstration"
                                                 style="width: 100%; margin-top: 0px;">
                                        </div>
                                    </div>
                                </dd>
                            </dl>
                        </div>
                        
                        <div>
                            <a class="anchor" id="9851517"></a>
                            <dl class="row">
                                <h3 class="col-lg-12 col-xl-1">2022</h3>
                                <dt class="col-1" style="text-align:right;">J1.</dt>
                                <dd class="col-md-11 col-xl-10">
                                    <div class="row">
                                        <!-- Main content column (4/5 width) -->
                                        <div class="col-8">
                                            <p>
                                                <font class="title">A Reinforcement Learning Method for Motion Control With Constraints on an HPN Arm</font>
                                                <br>
                                                <nobr>Yinghao Gan</nobr>, <nobr>Peijin Li</nobr>, <nobr>Hao Jiang</nobr>, <nobr style="font-weight: bold;">Gaotian Wang</nobr>, <nobr>Yusong Jin</nobr>, <nobr>Xiaoping Chen</nobr>, and <nobr>Jianmin Ji</nobr>
                                                <br>
                                                <em>IEEE Robotics and Automation Letters (RAL)</em>
                                            </p>
                                            <p>
                                                <a href="#9851517bibtex" data-toggle="collapse">Bibtex</a> <b>/ </b>
                                                <a href="#9851517abstract" data-toggle="collapse">Abstract</a> <b>/ </b>
                                                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9851517"><i class="fa fa-file-pdf"></i> PDF</a> <b>/ </b>
                                                <a href="https://ieeexplore.ieee.org/abstract/document/9851517"><i class="ai ai-doi"></i> Publisher</a>
                                            </p>
                                            <div id="9851517abstract" class="collapse">
                                                <div class="well">
                                                    <p>Soft robotic arms have shown great potential toward applications to human daily lives, which is mainly due to their infinite passive degrees of freedom and intrinsic safety. There are tasks in lives that require the motion of the robot to meet some certain pose constraints that have not been implemented through the soft arm, like delivering a glass of water. Because the workspace of the soft arm is affected by the loads or interaction, it is difficult to implement this task through the motion planning method. In this letter, we propose a Q-learning based approach to address the problem, directly achieving motion control with constraints under loads and interaction without planning. We first generate a controller for the soft arm based on Q-learning, which can operate the arm while satisfying the pose constraints when the arm is neither loaded nor interacted with the environment. Then, we introduce a process that adjusts corresponding Q values in the controller, which allows the controller to operate the arm with an unknown load or interaction while still satisfying the pose constraints. We implement the approach on our soft arm, i.e., the Honeycomb Pneumatic Network (HPN) Arm. The experiments show that the approach is effective, even when the arm reached an untrained situation or even beyond the workspace under the interaction.</p>
                                                </div>
                                                <p><a href="#9851517abstract" data-toggle="collapse">Close</a></p>
                                            </div>
                                            <div id="9851517bibtex" class="collapse">
                                                <div class="well">
                                                    <pre>@ARTICLE{9851517,
    author={Gan, Yinghao and Li, Peijin and Jiang, Hao and Wang, Gaotian and Jin, Yusong and Chen, Xiaoping and Ji, Jianmin},
    journal={IEEE Robotics and Automation Letters}, 
    title={A Reinforcement Learning Method for Motion Control With Constraints on an HPN Arm}, 
    year={2022},
    volume={7},
    number={4},
    pages={12006-12013},
    keywords={Motion control;Q-learning;Manipulators;Task analysis;Soft robotics;Load modeling;Data models;Machine learning for robot control;modeling;control;and learning for soft robots;soft robot applications},
    doi={10.1109/LRA.2022.3196789}}
                                              
                                              </pre>
                                                </div>
                                            </div>
                                        </div>
                                        <!-- GIF column (1/5 width) -->
                                        <div class="col-4">
                                            <img src="images/SoftRL.jpg" 
                                                 class="img-fluid" 
                                                 alt="HPN Arm demonstration"
                                                 style="width: 100%; margin-top: 0px;">
                                        </div>
                                    </div>
                                </dd>
                            </dl>
                        </div>
                        <h2>Peer-Reviewed Conference Articles</h2>
                        <div>
                            
                            <dl class="row">
                                <h3 class="col-lg-12 col-xl-1">2024</h3>
                                <dt class="col-1" style="text-align:right;">C5.</dt>
                                <dd class="col-md-11 col-xl-10">
                                    <div class="row">
                                        <!-- Main content column (4/5 width) -->
                                        <div class="col-8">
                                            <p>
                                                <a class="anchor" id="wang_uno_2024"></a>
                                                <font class="title">UNO Push: Unified Nonprehensile Object Pushing via Non-Parametric Estimation and Model Predictive Control</font>
                                                <br>
                                                <nobr style="font-weight: bold;">Gaotian Wang</nobr>, <nobr>Kejia Ren</nobr>, <nobr>Kaiyu Hang</nobr>
                                                <br>
                                                <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>
                                            </p>
                                            <p>
                                                <a href="#wang_uno_2024bibtex" data-toggle="collapse">Bibtex</a> <b>/ </b>
                                                <a href="#wang_uno_2024abstract" data-toggle="collapse">Abstract</a> <b>/ </b>
                                                <a href="https://arxiv.org/abs/2403.13274"><i class="fa fa-file-pdf"></i> PDF</a> <b>/ </b>
                                                <a href="https://www.youtube.com/watch?v=MLzGTxhKs6E&t=1s"> <i class="fa fa-video"></i> Video </a>
                                            </p>
                                            <div id="wang_uno_2024abstract" class="collapse">
                                                <div class="well">
                                                    <p>Nonprehensile manipulation through precise pushing is an essential skill that has been commonly challenged by perception and physical uncertainties, such as those associated with contacts, object geometries, and physical properties. For this, we propose a unified framework that jointly addresses system modeling, action generation, and control. While most existing approaches either heavily rely on a priori system information for analytic modeling, or leverage a large dataset to learn dynamic models, our framework approximates a system transition function via non-parametric learning only using a small number of exploratory actions (ca. 10). The approximated function is then integrated with model predictive control to provide precise pushing manipulation. Furthermore, we show that the approximated system transition functions can be robustly transferred across novel objects while being online updated to continuously improve the manipulation accuracy. Through extensive experiments on a real robot platform with a set of novel objects and comparing against a state-of-the-art baseline, we show that the proposed unified framework is a light-weight and highly effective approach to enable precise pushing manipulation all by itself. Our evaluation results illustrate that the system can robustly ensure millimeter-level precision and can straightforwardly work on any novel object.</p>
                                                </div>
                                                <p><a href="#wang_uno_2024abstract" data-toggle="collapse">Close</a></p>
                                            </div>
                                            <div id="wang_uno_2024bibtex" class="collapse">
                                                <div class="well">
                                                    <pre>@misc{wang_push_2024,
                                title = {{UNO} Push: Unified Nonprehensile Object Pushing via Non-Parametric Estimation and Model Predictive Control},
                                url = {http://arxiv.org/abs/2403.13274},
                                author = {Wang, Gaotian and Ren, Kejia and Hang, Kaiyu},
                                date = {2024-03-19},
                                eprint = {2403.13274 [cs]},
                                publisher = {arXiv},
                                langid = {english},
                            }
                                            </pre>
                                                </div>
                                            </div>
                                        </div>
                                        <!-- GIF column (1/5 width) -->
                                        <div class="col-4">
                                            <img src="images/uno.gif" 
                                                 class="img-fluid" 
                                                 alt="UNO Push demonstration"
                                                 style="width: 100%; margin-top: 0px;">
                                        </div>
                                    </div>
                                </dd>

                                <h3 class="col-lg-12 col-xl-1"></h3>
                                <dt class="col-1" style="text-align:right;">C4.</dt>
                                <dd class="col-md-11 col-xl-10">
                                    <div class="row">
                                        <!-- Main content column (4/5 width) -->
                                        <div class="col-8">
                                            <p>
                                                <a class="anchor" id="qian_riseg_nodate"></a>
                                                <font class="title">RISeg: Robot Interactive Object Segmentation via Body Frame-Invariant Features</font>
                                                <br>
                                                <nobr>Howard Qian</nobr>, <nobr>Yangxiao Lu</nobr>, <nobr>Kejia Ren</nobr>, <nobr style="font-weight: bold;">Gaotian Wang</nobr>, <nobr>Ninad Khargonkar</nobr>, <nobr>Yu Xiang</nobr>, <nobr>Kaiyu Hang</nobr>
                                                <br>
                                                <em>IEEE International Conference on Robotics and Automation (ICRA)</em>
                                            </p>
                                            <p>
                                                <a href="#qian_riseg_nodatebibtex" data-toggle="collapse">Bibtex</a> <b>/ </b>
                                                <a href="#qian_riseg_nodateabstract" data-toggle="collapse">Abstract</a> <b>/ </b>
                                                <a href="https://arxiv.org/abs/2403.01731"><i class="fa fa-file-pdf"></i> PDF</a> <b>/ </b>
                                                <a href="https://www.youtube.com/watch?v=K_FU310Jm1k"> <i class="fa fa-video"></i> Video </a>
                                            </p>
                                            <div id="qian_riseg_nodateabstract" class="collapse">
                                                <div class="well">
                                                    <p>In order to successfully perform manipulation tasks in new environments, such as grasping, robots must be proficient in segmenting unseen objects from the background and/or other objects. Previous works perform unseen object instance segmentation (UOIS) by training deep neural networks on large-scale data to learn RGB/RGB-D feature embeddings, where cluttered environments often result in inaccurate segmentations. We build upon these methods and introduce a novel approach to correct inaccurate segmentation, such as under-segmentation, of static image-based UOIS masks by using robot interaction and a designed body frame-invariant feature. We demonstrate that the relative linear and rotational velocities of frames randomly attached to rigid bodies due to robot interactions can be used to identify objects and accumulate corrected object-level segmentation masks. By introducing motion to regions of segmentation uncertainty, we are able to drastically improve segmentation accuracy in an uncertainty-driven manner with minimal, non-disruptive interactions (ca. 2-3 per scene). We demonstrate the effectiveness of our proposed interactive perception pipeline in accurately segmenting cluttered scenes by achieving an average object segmentation accuracy rate of $80.7 \%$, an increase of $28.2 \%$ when compared with other state-of-the-art UOIS methods.</p>
                                                </div>
                                                <p><a href="#qian_riseg_nodateabstract" data-toggle="collapse">Close</a></p>
                                            </div>
                                            <div id="qian_riseg_nodatebibtex" class="collapse">
                                                <div class="well">
                                                    <pre>@article{qian_riseg_nodate,
    title = {{RISeg}: Robot Interactive Object Segmentation via Body Frame-Invariant Features},
    author = {Qian, Howard and Lu, Yangxiao and Ren, Kejia and Wang, Gaotian and Khargonkar, Ninad and Xiang, Yu and Hang, Kaiyu},
    langid = {english},
    file = {Qian et al. - RISeg Robot Interactive Object Segmentation via B.pdf:C\:\\Users\\Vector Wang\\Zotero\\storage\\AH2HDVLC\\Qian et al. - RISeg Robot Interactive Object Segmentation via B.pdf:application/pdf},
}
                                              </pre>
                                                </div>
                                            </div>
                                        </div>
                                        <!-- GIF column (1/5 width) -->
                                        <div class="col-4">
                                            <img src="images/riseg.gif" 
                                                 class="img-fluid" 
                                                 alt="RISeg demonstration"
                                                 style="width: 100%; margin-top: 0px;">
                                        </div>
                                    </div>
                                </dd>

                                <h3 class="col-lg-12 col-xl-1"></h3>
                                <dt class="col-1" style="text-align:right;">C3.</dt>
                                <dd class="col-md-11 col-xl-10">
                                    <div class="row">
                                        <!-- Main content column (4/5 width) -->
                                        <div class="col-8">
                                            <p>
                                                <a class="anchor" id="wang_kinematic_nodate"></a>
                                                <font class="title">Kinematic Modeling and Control of a Soft Robotic Arm with Non-constant Curvature Deformation</font>
                                                <br>
                                                <nobr>Zhanchi Wang</nobr>, <nobr style="font-weight: bold;">Gaotian Wang</nobr>, <nobr>Xiaoping Chen</nobr>, and <nobr>Nikolaos M Freris</nobr>
                                                <br>
                                                <em>IEEE International Conference on Robotics and Automation (ICRA)</em>
                                            </p>
                                            <p>
                                                <a href="#wang_kinematic_nodatebibtex" data-toggle="collapse">Bibtex</a> <b>/ </b>
                                                <a href="https://ieeexplore.ieee.org/document/10611049"><i class="fa fa-file-pdf"></i> PDF</a> <b>/ </b>
                                                <a href="#wang_kinematic_nodateabstract" data-toggle="collapse">Abstract</a> 
                                            </p>
                                            <div id="wang_kinematic_nodateabstract" class="collapse">
                                                <div class="well">
                                                    <p>The passive compliance of soft robotic arms renders the development of accurate kinematic models and modelbased controllers challenging. The most widely used model in soft robotic kinematics assumes Piecewise Constant Curvature (PCC). However, PCC introduces errors when the robot is subject to external forces or even gravity. In this paper, we establish a three-dimensional (3D) kinematic representation of a soft robotic arm with pseudo universal and prismatic joints that are capable of capturing non-constant curvature deformations of the soft segments. We theoretically demonstrate that this constitutes a more general methodology than PCC. Simulations and experiments on the real robot attest to the superior modeling accuracy of our approach in 3D motions with unknown loads. The maximum position/rotation error of the proposed model is verified $6.7 \times / 4.6 \times$ lower than the PCC model considering gravity and external forces. Furthermore, we devise an inverse kinematic controller that is capable of positioning the tip, tracking trajectories, as well as performing interactive tasks in the 3D space.</p>
                                                </div>
                                                <p><a href="#wang_kinematic_nodateabstract" data-toggle="collapse">Close</a></p>
                                            </div>
                                            <div id="wang_kinematic_nodatebibtex" class="collapse">
                                                <div class="well">
                                                    <pre>@article{wang_kinematic_nodate,
        title = {Kinematic Modeling and Control of a Soft Robotic Arm with Non-constant Curvature Deformation},
        author = {Wang, Zhanchi and Wang, Gaotian and Chen, Xiaoping and Freris, Nikolaos},
        langid = {english},
        file = {Wang et al. - Kinematic Modeling and Control of a Soft Robotic A.pdf:C\:\\Users\\Vector Wang\\Zotero\\storage\\2N79EKKQ\\Wang et al. - Kinematic Modeling and Control of a Soft Robotic A.pdf:application/pdf},
    }
                                              </pre>
                                                </div>
                                            </div>
                                        </div>
                                        <!-- GIF column (1/5 width) -->
                                        <div class="col-4">
                                            <img src="images/soft.gif" 
                                                 class="img-fluid" 
                                                 alt="Soft Robot demonstration"
                                                 style="width: 100%; margin-top: 0px;">
                                        </div>
                                    </div>
                                </dd>
                            </dl>
                        </div>
                        <div>
                            <a class="anchor" id="10354732"></a>
                            <dl class="row">
                                <h3 class="col-lg-12 col-xl-1">2023</h3>
                                <dt class="col-1" style="text-align:right;">C2.</dt>
                                <dd class="col-md-11 col-xl-10">
                                    <div class="row">
                                        <!-- Main content column (4/5 width) -->
                                        <div class="col-8">
                                            <p>
                                                <font class="title">Dynamic modeling and Control of a Soft Robotic Arm Using a Piecewise Universal Joint Model</font>
                                                <br>
                                                <nobr>Zhanchi Wang</nobr>, <nobr style="font-weight: bold;">Gaotian Wang</nobr>, <nobr>Xiaoping Chen</nobr>, and <nobr>Nikolaos M Freris</nobr>
                                                <br>
                                                <em>IEEE International Conference on Robotics and Biomimetics (ROBIO)</em>
                                            </p>
                                            <p>
                                                <a href="#10354732bibtex" data-toggle="collapse">Bibtex</a> <b>/ </b>
                                                <a href="#10354732abstract" data-toggle="collapse">Abstract</a> <b>/ </b>
                                                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10354732"><i class="fa fa-file-pdf"></i> PDF</a> <b>/ </b>
                                                <a href="https://ieeexplore.ieee.org/abstract/document/10354732"><i class="ai ai-doi"></i> Publisher</a> <b>/ </b>
                                                <a href="videos/0202_VD_fi.mp4" > <i class="fa fa-video"></i> Video </a>
                                            </p>
                                            <div id="10354732abstract" class="collapse">
                                                <div class="well">
                                                    <p>The Piecewise Constant Curvature (PCC) assumption is the most widely used in the modeling and control of soft robots. However, a limitation of PCC models lies in accurately describing the deformation of a soft robot when executing dynamic tasks such as operating under gravity or interacting with humans. This paper introduces a new methodology for dynamic modeling and control of a multi-segment soft arm in the 3D space where each segment undergoes non-constant curvature deformations. In this framework, the soft manipulator is modeled as a series of segments, with each one represented by two stretchable links connected by a universal joint. Furthermore, we devise and analyze a controller for motion control in the configuration space with unknown load. The controller is implemented on a four-segment soft robotic manipulator and validated in a range of dynamic trajectory tracking tasks.</p>
                                                </div>
                                                <p><a href="#10354732abstract" data-toggle="collapse">Close</a></p>
                                            </div>
                                            <div id="10354732bibtex" class="collapse">
                                                <div class="well">
                                                    <pre>@INPROCEEDINGS{10354732,
    author={Wang, Zhanchi and Wang, Gaotian and Chen, Xiaoping and Freris, Nikolaos M.},
    booktitle={2023 IEEE International Conference on Robotics and Biomimetics (ROBIO)}, 
    title={Dynamic modeling and Control of a Soft Robotic Arm Using a Piecewise Universal Joint Model}, 
    year={2023},
    volume={},
    number={},
    pages={1-6},
    keywords={Solid modeling;Three-dimensional displays;Deformation;Motion segmentation;Dynamics;Soft robotics;Aerospace electronics},
    doi={10.1109/ROBIO58561.2023.10354732}}
                                              
                                              </pre>
                                                </div>
                                            </div>
                                        </div>
                                        <!-- GIF column (1/5 width) -->
                                        <div class="col-4">
                                            <img src="images/puj.gif" 
                                                 class="img-fluid" 
                                                 alt="Soft Robot demonstration"
                                                 style="width: 100%; margin-top: 0px;">
                                        </div>
                                    </div>
                                </dd>
                            </dl>
                        </div>
                        <div>
                            <a class="anchor" id="9739524"></a>
                            <dl class="row">
                                <h3 class="col-lg-12 col-xl-1">2021</h3>
                                <dt class="col-1" style="text-align:right;">C1.</dt>
                                <dd class="col-md-11 col-xl-10">
                                    <div class="row">
                                        <!-- Main content column (4/5 width) -->
                                        <div class="col-8">
                                            <p>
                                                <font class="title">A Q-learning Control Method for a Soft Robotic Arm Utilizing Training Data from a Rough Simulator</font>
                                                <br>
                                                <nobr>Peijin Li</nobr>, <nobr style="font-weight: bold;">Gaotian Wang</nobr>, <nobr>Hao Jiang</nobr>, <nobr>Yusong Jin</nobr>, <nobr>Yinghao Gan</nobr>, <nobr>Xiaoping Chen</nobr>, and <nobr>Jianmin Ji</nobr>
                                                <br>
                                                <em>IEEE International Conference on Robotics and Biomimetics (ROBIO)</em>
                                            </p>
                                            <p>
                                                <a href="#9739524bibtex" data-toggle="collapse">Bibtex</a> <b>/ </b>
                                                <a href="#9739524abstract" data-toggle="collapse">Abstract</a> <b>/ </b>
                                                <a href="https://arxiv.org/pdf/2109.05795.pdf"><i class="fa fa-file-pdf"></i> PDF</a> <b>/ </b>
                                                <a href="https://ieeexplore.ieee.org/abstract/document/9739524"><i class="ai ai-doi"></i> Publisher</a>
                                            </p>
                                            <div id="9739524abstract" class="collapse">
                                                <div class="well">
                                                    <p>It is challenging to control a soft robot, where reinforcement learning methods have been applied with promising results. However, due to the poor sample efficiency, reinforcement learning methods require a large collection of training data, which limits their applications. In this paper, we propose a Q-learning controller for a physical soft robot, in which pre-trained models using data from a rough simulator are applied to improve the performance of the controller. We implement the method on our soft robot, i.e., Honeycomb Pneumatic Network (HPN) arm. The experiments show that the usage of pre-trained models can not only reduce the amount of the real-world training data, but also greatly improve its accuracy and convergence rate.</p>
                                                </div>
                                                <p><a href="#9739524abstract" data-toggle="collapse">Close</a></p>
                                            </div>
                                            <div id="9739524bibtex" class="collapse">
                                                <div class="well">
                                                    <pre>@INPROCEEDINGS{9739524,
    author={Li, Peijin and Wang, Gaotian and Jiang, Hao and Jin, Yusong and Gan, Yinghao and Chen, Xiaoping and Ji, Jianmin},
    booktitle={2021 IEEE International Conference on Robotics and Biomimetics (ROBIO)}, 
    title={A Q-learning Control Method for a Soft Robotic Arm Utilizing Training Data from a Rough Simulator}, 
    year={2021},
    volume={},
    number={},
    pages={839-845},
    keywords={Q-learning;Costs;Conferences;Biomimetics;Biological system modeling;Training data;Soft robotics},
    doi={10.1109/ROBIO54168.2021.9739524}}
                                              </pre>
                                                </div>
                                            </div>
                                        </div>
                                        <!-- GIF column (1/5 width) -->
                                        <div class="col-4">
                                            <img src="images/softq.jpg" 
                                                 class="img-fluid" 
                                                 alt="Soft Robot RL demonstration"
                                                 style="width: 100%; margin-top: 0px;">
                                        </div>
                                    </div>
                                </dd>
                            </dl>
                        </div>
                    </div>
                    <hr>


                    <!-- Experience Section -->
                    <a class="anchor" id="experience"></a>
                    <h1>Experience üßó</h1>
                    <div class="row">
                        <!-- Education Column -->
                        <div class="col-md-6">
                            <h2>Education</h2>
                            <div style="padding: 10px">
                                <dt>Rice University | 2022 - Present</dt>
                                <dt><em><small>Houston, TX</small></em></dt>
                                <dd>
                                    <ul>
                                        <li>Ph.D. in Computer Science</li>
                                        <li>Advisor: <a href="https://hangkaiyu.github.io/">Dr. Kaiyu Hang</a></li>
                                    </ul>
                                </dd>
                                <dt>University of Science and Technology of China</dt>
                                <dt>USTC | 2018 - 2022</dt>
                                <dt><em><small>Hefei, China</small></em></dt>
                                <ul>
                                    <li>B.S. in Optical Engineering</li>
                                    <li>Thesis: <em>A Randomized Kinodynamic Planner for Soft Robots based on Piecewise Universal Joint Model</em></li>
                                    <li>Advisor: <a href="http://staff.ustc.edu.cn/~nfr/">Dr. Nikolaos M. Freris</a></li>
                                </ul>
                            </div>
                        </div>
                        <!-- Research Column -->
                        <div class="col-md-6">
                            <h2>Research</h2>
                            <div style="padding: 10px">
                                <dt><a href="https://robotpilab.github.io/">Robot‚àè Lab</a> | Sep.2022 - Present</dt>
                                <dt><em><small>Rice University, Houston, TX</small></em></dt>
                                <dd>
                                    <ul>
                                        <li>Graduate Student</li>
                                        <li>Advisor: Dr. Kaiyu Hang</li>
                                    </ul>
                                </dd>
                                <dt><a href="http://staff.ustc.edu.cn/~nfr/Flyer%20-%20AIoT%20-%20English.pdf">AIoT Lab</a> | Nov.2021 - Jun.2022</dt>
                                <dt><em><small>USTC, Hefei, China</small></em></dt>
                                <dd>
                                    <ul>
                                        <li>Undergraduate Researcher</li>
                                        <li>Advisor: Dr. Nikolaos M. Freris</li>
                                    </ul>
                                </dd>
                                <dt><a href="https://www.epfl.ch/labs/rrl/">Reconfigurable Robotics Lab</a> | Apr.2021 - Sep.2021</dt>
                                <dt><em><small>EPFL, Lausanne ,Switzerland</small></em></dt>
                                <dd>
                                    <ul>
                                        <li>Guest Researcher</li>
                                        <li>Supervisor: Dr. Fabio Zuliani and Dr. Jamie Paik</li>
                                    </ul>
                                </dd>
                                <dt><a href="https://sites.google.com/view/softrobot">USTC Soft Robotics Lab</a> | Aug.2020 - Oct.2021</dt>
                                <dt><em><small>USTC, Hefei, China</small></em></dt>
                                <dd>
                                    <ul>
                                        <li>Undergraduate Researcher</li>
                                        <li>Advisor: Dr. Hao Jiang and Dr. Xiaoping Chen</li>
                                    </ul>
                                </dd>
                                
                            </div>
                        </div>
                    </div>
                    <hr>


                    <!-- <h1>Work Experience</h1>-->
                    <a class="anchor" id="teaching"></a> 
                    <h1>Teaching üßë‚Äçüè´</h1>
                    <div style="padding: 10px">
                        <dl>
                            <dd>Teaching Assistant for <a href="https://piazza.com/class/llmhssuxiuj3i2"><b>Algorithmic Robotics</b></a> | Fall 2023
                                <br>
                                <em><small>COMP/ELEC/MECH 450/550 at Rice University</small></em>
                            </dd>
                            <dd>Teaching Assistant for <a href="https://www.cs.rice.edu/~vo9/deep-vislang/"><b>Deep Learning for Vision & Language</b></a> | Spring 2023
                                <br>
                                <em><small>COMP 646 at Rice University</small></em>
                            </dd>
                            <dd>In-lab Teaching Assistant for <a href="https://jxzy.ustc.edu.cn/#"><b>College Physics-Comprehensive Experimentation</b></a> | Fall 2020-2022
                                <br>
                                <em><small>at University of Science and Technology of China</small></em>
                            </dd>
                        </dl>
                    </div>
                    <hr>
                    <a class="anchor" id="Hobbies"></a> 
                    <h1>Life üèîÔ∏èüßó‚ÄçüèÇüõπüèúÔ∏èüóΩ‚õπÔ∏èüöÄ</h1>
                    We bring robots to life, but life is more than robots.
                    <div style="display: flex; justify-content: space-around;">
                        <div class="slider" style="display: flex; justify-content: center; align-items: center; position: relative; width: 500px; height: 600px;">
                            <img id="slide1" src="images/3.jpg" style="position: absolute; transition: opacity 1s; opacity: 1;max-height: 95%;">
                            <img id="slide2" src="images/4.jpg" style="position: absolute; transition: opacity 1s; opacity: 0; max-height: 95%;">
                        </div>
                        <div class="slider" style="display: flex; justify-content: center; align-items: center; position: relative; width: 500px; height: 600px;">
                            <img id="slide3" src="images/1.jpg" style="position: absolute; transition: opacity 1s; opacity: 1;max-height: 95%;">
                            <img id="slide4" src="images/2.jpg" style="position: absolute; transition: opacity 1s; opacity: 0; max-height: 95%;">
                        </div>
                    </div>
                    
                    <script>
                        var images = ["images/0.jpg", "images/1.jpg", "images/2.jpg", "images/3.jpg", "images/4.jpg", "images/5.jpg", "images/6.jpg", "images/7.jpg", "images/8.jpg", "images/9.jpg", "images/10.jpg", "images/11.jpg", "images/12.jpg", "images/13.jpg", "images/14.jpg", "images/15.jpg", "images/16.jpg", "images/17.jpg", "images/18.jpg", "images/19.jpg", "images/20.jpg", "images/21.jpg", "images/22.jpg", "images/23.jpg", "images/24.jpg", "images/25.jpg", "images/26.jpg", "images/27.jpg", "images/28.jpg", "images/29.jpg",]; // replace with your images
                        var index = 0;
                    
                        function slide(slide1Id, slide2Id) {
                            var slide1 = document.getElementById(slide1Id);
                            var slide2 = document.getElementById(slide2Id);
                    
                            slide1.src = images[index];
                            slide2.src = images[(index + 1) % images.length];
                    
                            slide1.style.opacity = 1;
                            slide2.style.opacity = 0;
                    
                            setTimeout(function() {
                                slide2.style.opacity = 1;
                                slide1.style.opacity = 0;
                            }, 200);
                    
                            index = (index + 1) % images.length;
                        }
                    
                        setInterval(function() {
                            slide("slide1", "slide2");
                            slide("slide3", "slide4");
                        }, 3000); // change image every 3 seconds
                    </script>
                </div>
                <br>
                <br>
                <br>
                <br>
                <br>
                <br>
                <br>
                <br>
                <hr>
                <div class="col-12 col-xs-4 col-md-3 col-lg-3 col-xl-2">
                    <div class="row">
                        <div class="col-3 col-xs-0 col-md-0 col-lg-0 col-xl-0">
                        </div>
                        <div class="col-6 col-xs-12 col-md-12 col-lg-12 col-xl-12">
                            <p>
                                <img src   = "images/Gaotian_.jpg"
                                     class = "img-fluid full-width"
                                     alt   = "A picture of me from February 2024.">
                            </p>
                        </div>
                        <div class="col-3 col-xs-0 col-md-0 col-lg-0 col-xl-0">
                        </div>
                        <div class="col-3 col-xs-0 col-md-0 col-lg-0 col-xl-0">
                        </div>
                        <div class="col-6 col-xs-12 col-md-12 col-lg-12 col-xl-12">
                            <h2>Contact</h2>
                            <dl class="row">
                                <dd class="col-10">gwang<i class="fa fa-at fa-xs"></i>rice.edu<br></dd>
                            </dl>
                            <dl class="row">
                                <dt class="col-3"><a href="https://www.linkedin.com/in/gaotian-wang/"><i class="fab fa-linkedin fa-2x"></i></a></dt>
                                <dt class="col-3"><a href="https://github.com/Vector-Wangel"><i class="fab fa-github fa-2x"></i></a></dt>
                                <dt class="col-3"><a href="https://scholar.google.com/citations?user=RuGkkZ8AAAAJ&hl=en"><i class="fab fa-google-scholar fa-2x"></i></a></dt>
                            </dl>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <div class="container">
            <footer>
                <p></p>
                <p style="line-height:normal;">Website design inspired by <a href="https://zkingston.com/">Zak Kingston</a></p>
            </footer>
        </div>
        <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
    </body>
</html>
